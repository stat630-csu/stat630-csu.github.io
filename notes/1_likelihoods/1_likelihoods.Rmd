---
title: "Likelihoods"
output:
  pagedown::html_paged:
    # change to true for a self-contained document, but it'll be a litte slower for Pandoc to render
    css: ["../style/my-style-page.css", "default-fonts", "default-page", "default"]
    self_contained: true
    number_sections: true
---

```{r, echo = FALSE, message = FALSE}
library(knitr)
library(ggplot2)
library(dplyr)
library(tidyr)
set.seed(100)

theme_set(theme_bw())
knitr::opts_chunk$set(fig.height = 3)
```

History of the course:

<br/><br/><br/><br/><br/><br/><br/><br/><br/><br/>

## Outline


# Likelihood Construction and Estimation

<br/><br/><br/><br/><br/>

Why do Statisticians love likelihood-based estimation?

1. <br/><br/><br/><br/>

2. <br/><br/><br/><br/>

3. <br/><br/><br/><br/>

4. <br/><br/><br/><br/>

Downsides?

1. <br/><br/><br/><br/>


1. <br/><br/>

## Introduction

**Definition: ** Suppose random variables $\boldsymbol Y = (Y_1, \dots Y_n)^\top$ has joint density or probability mass function $f_\boldsymbol Y(\boldsymbol y, \boldsymbol \theta)$ where $\boldsymbol \theta = (\theta_1, \dots, \theta_b)$. Then the *likelihood function* is
$$
L(\boldsymbol \theta | \boldsymbol Y) = f_\boldsymbol Y(\boldsymbol Y, \boldsymbol \theta).
$$

<br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/>

**Key concept: ** In all situations, the likelihood is the joint density of the observed data to be analyzed.

<br/><br/><br/><br/><br/><br/><br/><br/><br/><br/>

### Notation

Given $\boldsymbol y$, note that $L(\boldsymbol \theta | \boldsymbol y): \mathbb{R}^b \rightarrow \mathbb{R}$.

<br/><br/><br/><br/><br/><br/><br/><br/>

Generally, we optimize $\ell(\boldsymbol \theta) = \log L(\boldsymbol \theta | \boldsymbol y)$.

<br/><br/><br/><br/>

How?

<br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/>

**Example:** Suppose we have $Y_1, \dots Y_n \stackrel{iid}{\sim} \text{Exp}(\lambda)$. The likelihood function is defined as

<br/><br/><br/><br/>

```{r}
# likelihood simulation
n <- 10
lambda <- 1

# plot of exponential(lambda) density
data.frame(x = seq(0, 8, .01)) |>
  mutate(f = dexp(x, rate = lambda)) |>
  ggplot() +
  geom_line(aes(x, f))
```

<br/><br/><br/><br/><br/><br/><br/><br/>

```{r}
# define likelihood
loglik <- function(lambda, data)
{
	lik <- prod(dexp(data, rate = lambda))
	loglik <- sum(dexp(data, rate = lambda, log = T))
	
	out <- data.frame(lik = lik, loglik = loglik)
	return(out)
}	

# simulate data
data <- rexp(n = n, rate = lambda)

# plot likelihood and loglikelihood
data.frame(lambda = seq(0, 3, by = .01)) |>
  rowwise() |>
  mutate(loglik = loglik(lambda, data)) |>
  unnest(cols = c(loglik)) |>
  pivot_longer(-lambda, names_to = "func", values_to = "vals") |>
  ggplot() +
  geom_vline(aes(xintercept = 1 / mean(data)), lty = 2) + # max likelihood estimate is 1/mean
  geom_line(aes(lambda, vals)) +
  facet_wrap(~func, scales = "free")

```
<br/><br/><br/><br/><br/><br/><br/><br/>

The likelihood function is random!

```{r, out.width="33%", fig.show='hold', fig.height=4, fig.width=4}
for(i in seq_len(3)) {
  # simulate data
  data <- rexp(n = n, rate = lambda)
  
  # plot likelihood and loglikelihood
  data.frame(lambda = seq(0, 3, by = .01)) |>
    rowwise() |>
    mutate(loglik = loglik(lambda, data)) |>
    unnest(cols = c(loglik)) |>
    ggplot() +
    geom_vline(aes(xintercept = 1 / mean(data)), lty = 2) + # max likelihood estimate is 1/mean
    geom_line(aes(lambda, loglik)) +
    theme(text = element_text(size = 20)) -> p ## make legible in notes
  
    print(p)
}

```

<br/><br/>

**Your Turn:** What is the effect of sample size on the log-likelihood function? Make a plot showing the log-likelihood function that results from $n = 10$ vs. $n = 100$ with corresponding MLE.

<br/><br/><br/><br/><br/><br/>

## Construction

The use of the likelihood function in parameter estimation is easiest to understand in the case of discrete iid random variables.

### Discrete IID Random Variables

Suppose each of the $n$ random variables in the sample $Y_1, \dots, Y_n$ have probability mass function $f(y; \boldsymbol \theta) = P_\boldsymbol \theta(Y_1 = y), y = y_1, y_2, \dots$. The likelihood is then defined as:
$$
L(\boldsymbol \theta | \boldsymbol Y) = \text{ joint density of observed random variables}
$$
<br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/>

In other words, 

<br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/>

**Example (Fetal Lamb Movements): ** Data on counts of movements in five-second intervals of one fetal lamb ($n = 240$ intervals:)

```{r, echo=FALSE}
data.frame(no_movement = seq(0, 7),
           count = c(182, 41, 12, 2, 2, 0, 0, 1)) |>
  t() -> lambs
rownames(lambs) <- c("No. of Movements", "Count")

lambs |>
  kable(booktabs = TRUE)
```

Assume a Poisson model: $P(Y = y) = f_Y(y; \lambda) = \frac{\exp(-\lambda)\lambda^y}{y!}$. Then the likelihood is

<br/><br/><br/><br/><br/><br/>

Equating the derivative of the loglikelihood with respect to $\lambda$ to zero and solving results in the MLE

$$
\hat{\lambda}_\text{MLE} = \qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad
$$
<br/><br/>

This is the best we can do with this model. But is it good?

```{r, echo = FALSE}
data.frame(no_movement = seq(0, 7),
           count = c(182, 41, 12, 2, 2, 0, 0, 1)) -> lambs_model_df

lambda_hat <- lambs_model_df |>
  mutate(exp_count = no_movement * count) |>
  summarise(lambda_hat = sum(exp_count) / sum(count)) |>
  pull(lambda_hat)

lambs_model_df |>
  rowwise() |>
  mutate(Model = dpois(no_movement, lambda_hat) * sum(lambs_model_df$count)) |>
  rename(Observed = count) |>
  pivot_longer(-no_movement, names_to = "type", values_to = "count") |>
  ggplot() +
  geom_bar(aes(no_movement, count, fill = type), position = "dodge", stat = "identity") +
  xlab("# of movements") +
  scale_fill_discrete("")
              

```
<br/><br/><br/><br/><br/><br/><br/><br/><br/><br/>

### Multinomial Likelihoods

The multinomial distribution is a generalization of the binomial distribution where instead of 2 outcomes (success or failure), there are now $k \ge 2$ outcomes.

<br/><br/><br/><br/>

The probability mass function is

<br/><br/><br/><br/>

For $N_1, \dots, N_k, N_i =$ the number of balls in $i^\text{th}$ urn,

<br/><br/><br/><br/><br/><br/><br/><br/><br/>

The maximum likelihood estimator of $p_i$:

<br/><br/><br/><br/><br/><br/><br/>

More interesting multinomial likelihoods arise when the $p_i$ are modeled as a function of a lesser number of parameters $\theta_1, \dots, \theta_m$, $m < k - 1$.

**Example (Capture-Recapture):** To estimate fish survival during a specific length of time (e.g., one month), a common approach is to use a removal design.

<br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/>
<br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/>
<br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/>


### Continuous IID Random Variables

Recall: the likelihood is the joint density of data to be analyzed.

**Example (Hurricane Data):** For $36$ hurricanes that had moved far inland on the East Coast of the US in 1900-1969, maximum 24-hour precipitation levels during the time they were over mountains.

```{r echo=FALSE}
hurr_rain <- c(31, 2.82, 3.98, 4.02, 9.5, 4.5, 11.4, 10.71, 6.31, 4.95, 5.64, 5.51, 13.40, 9.72, 6.47, 10.16, 4.21, 11.6, 4.75, 6.85, 6.25, 3.42, 11.8, 0.8, 3.69, 3.10, 22.22, 7.43, 5, 4.58, 4.46, 8, 3.73, 3.5, 6.2, 0.67)

ggplot() + 
  geom_histogram(aes(hurr_rain), binwidth = .5) +
  xlab("Precipitation Levels")
```

We model the precipitation levels with a gamma distribution, which has density
$$
f(y; \alpha, \beta) = \frac{1}{\Gamma(\alpha) \beta^\alpha} y^{\alpha - 1}\exp(-y/\beta), \quad y > 0, \alpha, \beta > 0.
$$
This leads to the likelihood

<br/><br/><br/><br/><br/>

Of course, this cannot be interpreted as a probability because

<br/><br/><br/><br/>

To get a probability, need to go from a density to a measure.

<br/><br/><br/><br/>

But it may be useful to think of the value of the likelihood as being proportional to a probability.

<br/><br/><br/><br/><br/><br/><br/><br/>

More formally, begin with the definition of a derivative

$$
g'(x) = \lim\limits_{h \rightarrow0^+} \frac{g(x + h) - g(x - h)}{2h}.
$$

Let $F$ be the cumulative distribution function of a continuous random variable $Y$, then (if the derivative exists)

$$
f(y) =  \lim\limits_{h \rightarrow0^+} \frac{F(x + h) - F(x - h)}{2h} = \qquad \qquad \qquad \qquad \qquad
$$

If we substitute this definition of a density into the definition of the likelihood

<br/><br/><br/><br/><br/><br/><br/><br/><br/><br/>

Compare this to the iid discrete case:


<br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/>

**Example (Hurricane Data, Cont'd):** Recall with a gamma model, the likelihood for this example is
$$
L(\boldsymbol \theta | \boldsymbol Y) = \{\Gamma(\alpha)\}^{-n} \beta^{-n\alpha} \left\{\prod Y_i\right\}^{\alpha - 1} \exp\left(-\sum y_i/\beta\right),
$$
and log-likelihood
$$
\ell(\boldsymbol \theta) = \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad
$$

```{r, fig.height=2}
## loglikelihood function
neg_gamma_loglik <- function(theta, data) {
  -sum(log(dgamma(data, theta[1], scale = theta[2])))
}

## maximize
mle <- nlm(neg_gamma_loglik, c(1.59, 4.458), data = hurr_rain)
mle$estimate

## Gamma QQ plot
data.frame(theoretical = qgamma(ppoints(hurr_rain), mle$estimate[1], scale = mle$estimate[2]),
           actual = sort(hurr_rain)) |>
  ggplot() +
  geom_abline(aes(intercept = 0, slope = 1), colour = "grey") +
  geom_point(aes(theoretical, actual)) +
  xlab("Gamma percentiles") + ylab("Ordered values")
  
```

### Mixtures of Discrete and Continuous RVs

Some data $Y$ often have a number of zeros and the amounts greater than zero are best modeled by a continuous distribution. 

<br/>

Ex:

<br/>

In other words, they have positive probability of taking a value of exactly zero, but continuous distribution otherwise.

<br/><br/><br/>

A sensible model would assume $Y_i$ are iid with cdf
$$
F_Y(y; p, \boldsymbol \theta) = \begin{cases}
0 & y = 0 \\
p & y = 0 \\
p + (1 - p)F_T(y; \boldsymbol \theta) & y > 0
\end{cases}
$$
where $0 < p \le 1$ is $P(Y = 0)$ and $F_T(y; \boldsymbol \theta)$ is a distribution function for a continuous positive random variable.

Another way to write this:

<br/><br/><br/><br/><br/>

How to go from here to get a likelihood?


<br/><br/><br/><br/><br/>

One approach: let $n_0$ be the number of zeroes in the data and $m = n - n_0$ be the number of non-zero $Y_i$. This leads to an intuitive way to contruct the likelihood for iid $Y_1, \dots, Y_n$ distributed according to the above distribution:
$$
L(\boldsymbol \theta | \boldsymbol Y) = \lim\limits_{h \rightarrow 0^+} \left(\frac{1}{2h}\right)^m \prod\limits_{i = 1}^n\{F_Y(Y_i + h; p, \boldsymbol \theta) - F_Y(Y_i - h; p, \boldsymbol \theta)\}
$$
<br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/>

Feels a little arbitrary in how we are defining different weights on our likelihood for discrete and continuous parts.

<br/><br/>

Turns out, it doesn't matter! (Need some STAT 630/720 to see why.)

**Definition (Absolute Continuity)** On $(\mathbb{X}, \mathcal{M})$, a finitely additive set function $\phi$ is *absolutely continuous* with respect to a measure $\mu$ if $\phi(A) = 0$ for each $A \in \mathcal{M}$ with $\mu(A) = 0$. We also say $\phi$ is *dominated* by $\mu$ and write $\phi \ll \mu$. If $\nu$ and $\mu$ are measures such that $\nu \ll \mu$ and $\mu \ll nu$ then $\mu$ and $\nu$ are *equivalent*.

<br/><br/><br/><br/>

**Theorem (Lebesgue-Randon-Nikodym)** Assume that $\phi$ is a $\sigma$-finite countably additive set function and $\mu$ is a $\sigma$-finite measure. There exist unique $\sigma$-finite countably additive set functions $\phi_s$ and $\phi_{ac}$ such that $\phi = \phi_{ac} + \phi_s \ll \mu$, $\phi_s$ and $\mu$ are mutually singular and there exists a measurable extended real valued function $f$ such that
$$
\phi_{ac}(A) = \int_A f d\mu, \qquad \text{ for all } A \in \mathcal{M}.
$$
If $g$ is another such function, then $f = g$ a.e. wrt $\mu$. If $\phi \ll \mu$ then $\phi(A) = \int_Af d\mu$ for all $A \in \mathcal{M}$.

<br/>

**Definition (Radon-Nikodym Derivative)** $\phi = \phi_{ac} + \phi_s$ is called the *Lebesgue decomposition*. If $\phi \ll \mu$, then the density function $f$ is called the *Radon-Nikodym derivative* of $\phi$ wrt $\mu$.

<br/>

So what?

<br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/>



<br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/>

### Proportional Likelihoods

Likelihoods are equivalent for point estimation as long as they are proportional and the constant of proportionality does not depend on unknown parameters.

Why?

Consider if $Y_i, i = 1, \dots, n$ are iid continuous with density $f_Y(y ; \boldsymbol \theta)$ and $X_i = g(Y_i)$ where $g$ is increasing and continuously differentiable. Because $g$ is one-to-one, we can construct $Y_i$ from $X_i$ and vice versa.

<br/><br/><br/><br/>

More formally, the density of $X_i$ is $f_X(x; \boldsymbol \theta) = f_Y(h(x); \boldsymbol \theta) h'(x)$, where $h = g^{-1}$, and
$$
L(\boldsymbol \theta | \boldsymbol X) = \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad
$$

<br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/>

**Example (Likelihood Principle):** Consider data from two different sampling plans:

    $$
    L_1(p | \boldsymbol Y) = {12 \choose S} p^S (1 - p)^{12 - S}, \text{ where } S = \sum\limits_{i = 1}^n Y_i
    $$
2. A negative binomial experiment, i.e. run the experiment until three zeroes are obtained.
    $$
    L_2(p | \boldsymbol Y) = {S + 2 \choose S} p^S (1 - p)^3.
    $$
The ratio of these likelihoods is
$$
\frac{L_1(p | \boldsymbol Y)}{L_2(p | \boldsymbol Y)} = \qquad \qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad
$$

Suppose $S = 9$. Is all inference equivalent for these likelihoods? Debatable.

<br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/>

**The likelihood principle** states all the information about $\boldsymbol \theta$ from an experiment is contained in the actual observation $\boldsymbol y$. Two likelihood functions for $\boldsymbol \theta$ (from the same or different experiments) contain the same information about $\boldsymbol \theta$ is they are proportional.

### Empirical Distribution Function as MLE {.page-break-before}

Recall the empirical cdf:

Suppose $y_{(1)} \le y_{(2)} \le \cdots \le y_{(n)}$ are the order statistics of an iid sample from an unknown distribution function $F_Y$. Our goal is to estimate $F_Y$.

$$
\hat{F}_Y(y) = \frac{1}{n} \sum\limits_{i = 1}^n \mathbb{I}(y \ge y_{(i)})
$$
<br/><br/><br/><br/><br/><br/><br/><br/><br/><br/>

Is this a "good" estimator of $F_Y$?

<br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/>

Yes, because it's MLE.

Suppose $Y_1, \dots, Y_n$ are iid with distribution function $F(y)$. Here $F(y)$ is the unknown parameter.

<br/><br/><br/><br/><br/><br/><br/>

An approximate likelihood for $F$ is

$$
L_h(F | \boldsymbol Y) = \prod\limits_{i = 1}^n \{F(Y_i + h) - F(Y_i - h)\}
$$

### Censored Data {.page-break-before}

Censored data occur when the value is only partially known. This is different from *truncation*, in which the data does not include any values below (or above) a certain limit. 

For example, we might sample only hourseholds that have an income above a limit, $L_0$. If all incomes have distribution $F(x; \boldsymbol \theta)$, then for $y > L_0$,

<br/>
$$
P(Y_1 \le y | Y_1 > L_0) = \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad 
$$
<br/>
The likelihood is then

<br/><br/><br/><br/><br/><br/><br/>

#### Type I Censoring

Suppose a random variable $X$ is normally distributed with mean $\mu$ and variance $\sigma^2$, but whenever $X \le 0$, all we observe is that it is less than or equal to $0$. If the sample is set to $0$ in the censored cases, then define
$$
Y = \begin{cases}
0 & \text{ if } X \le 0 \\
X & \text{ if } X > 0.
\end{cases}
$$
The distribution function of $Y$ is

<br/><br/><br/><br/><br/><br/><br/>

Suppose we have a sample $Y_1, \dots, Y_n$ and let $n_0$ be the number of sample values that are $0$. Then $m = n - n_0$ and

<br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/>

We might have censoring on the left at $L_0$ and censoring on the right at $R_0$, but observe all values of $X$ between $L_0$ and $R_0$. Suppose $X$ has density $f(x; \boldsymbol \theta)$ and distribution function $F(x; \boldsymbol \theta)$ and
$$
Y_i = \begin{cases}
L_0 & \text{ if } X_i \le L_0 \\
X_i & \text{ if } L_0 < X_i < R_0 \\
R_0 & \text{ if } X_i \ge R_0 \\
\end{cases}
$$

If we let $n_L$ and $n_R$ be the number of $X_i$ values $\le L_0$ and $\ge R_0$ then the likelihood of the observed data $Y_1, \dots, Y_n$ is

<br/><br/><br/><br/><br/><br/>

We could also let each $X_i$ be subject to its own censoring values $L_i$ and $R_i$. For the special case of right censoring, define $Y_i = \min(X_i, R_i)$. In addition, define $\delta_i = \mathbb{I}(X_i \le R_i)$. Then the likelihood can be written as

<br/><br/><br/><br/><br/>

**Example (Equipment failure times):** Pieces of equipment are regularly checked for failure (but started at different times). By a fixed date (when the study ended), three of the items had not failed and therefore were censored.

```{r, echo=FALSE}
failure <- data.frame(y = c(2, 72, 51, 50, 33, 27, 14, 24, 4, 21),
                      delta = c(1, 0, 1, 0, 1, 1, 1, 1, 1, 0))

failure |>
  t() |>
  kable()
```
Suppose failure times follow an exponential distribution $F(x; \sigma) = 1 - \exp(-x/\sigma), x \ge 0$. Then
$$
L(\sigma|\boldsymbol Y) = \qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad
$$

#### Random Censoring {.page-break-before}

So far we have considered censoring times to be fixed. This is not required.

<br/><br/><br/><br/>

This leads to random censoring times, e.g. $R_i$, where we assume that the censoring times are independent of $X_1, \dots, X_n$ and iid with distribution function $G(t)$ nd density $g(t)$.

Let's consider the contributions to the likelihood:

<br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/>

which results in

$$
L(\boldsymbol \theta | \boldsymbol Y, \boldsymbol \delta) = \qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad
$$

## Likelihood for Regression Models {.page-break-before}

## Marginal and Conditional Likelihoods

## The Maximum Likelihood Estimator and the Information Matrix

## Methods for Maximizing the Likelihood


