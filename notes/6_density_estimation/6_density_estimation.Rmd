---
title: "Density Estimation"
output:
  pagedown::html_paged:
    css: ["../style/my-style-page.css", "default-fonts", "default-page", "default"]
    self_contained: true
    number_sections: true
---

```{r, echo = FALSE, message=FALSE, warning=FALSE}
library(knitr)
library(ggplot2)
library(dplyr)
library(tidyr)
set.seed(333)

theme_set(theme_bw())
knitr::opts_chunk$set(fig.height = 3)
```

**Goal:** We are interested in estimation of a density function $f$ using observations of random variables $Y_1, \dots, Y_n$ sampled independently from $f$.

<br/><br/><br/><br/><br/><br/><br/><br/><br/>

Parametric Solution: 

<br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/>

We will focus on **nonparametric** approaches to density estimation.

# Histograms {.page-break-before}

One familiar density estimator is a histogram. Histograms are produced automatically by most software packages and are used so routinely to visualize densities that we rarely talk about their underlying complexity.

## Motivation

Recall the definition of a density function
$$
f(y) \equiv \frac{d}{dy} F(y) \equiv \lim\limits_{h \rightarrow 0} \frac{F(y + h) - F(y - h)}{2h} = \lim\limits_{h \rightarrow 0} \frac{F(y + h) - F(y)}{h},
$$
where $F(x)$ is the cdf of the random variable $Y$.

Now, let $Y_1, \dots, Y_n$ be a random sample of size $n$ from the density $f$.

<br/><br/><br/><br/><br/><br/>

A natural finite-sample analog of $f(y)$ is to divide the support of $Y$ into a set of $K$ equi-sized bins with small width $h$ and replace $F(x)$ with the empirical cdf.

<br/><br/><br/><br/><br/><br/>

## Bin Width {.page-break-before}

```{r, echo = FALSE, fig.show='hold'}
samps <- data.frame(x1 = rnorm(100), x2 = rnorm(100), x3 = rnorm(100))
dens <- data.frame(x = seq(-3.5, 3.5, length.out = 1000)) |>
  rowwise() |>
  mutate(f = dnorm(x))

samps |>
  pivot_longer(everything(), names_to = "sample", values_to = "value") |>
  mutate(sample = factor(sample, labels = c("Sample 1", "Sample 2", "Sample 3"))) |>
  ggplot() +
  geom_histogram(aes(value, y = after_stat(density)), bins = 20) +
  geom_line(aes(x, f), data = dens, colour = "red") +
  facet_wrap(.~sample)

samps |>
  pivot_longer(everything(), names_to = "sample", values_to = "value") |>
  mutate(sample = factor(sample, labels = c("Sample 1", "Sample 2", "Sample 3"))) |>
  ggplot() +
  geom_histogram(aes(value, y = after_stat(density)), bins = 4) +
  geom_line(aes(x, f), data = dens, colour = "red") +
  facet_wrap(.~sample)
```

<br/><br/><br/><br/><br/><br/>

## Measures of Performance {.page-break-before}

Squared Error

<br/><br/><br/><br/>

Mean Squared Error

<br/><br/><br/><br/>

Integrated Squared Error

<br/><br/><br/><br/>

Mean Integrated Squared Error

<br/><br/><br/><br/><br/><br/><br/><br/>

## Optimal Binwidth {.page-break-before}

We will investigate bias and variance of $\hat f$ pointwise, because $\text{MSE}(y) = (\text{bias}(\hat f(y))^2 + \text{Var} \hat f(y)$.

[]{.pagebreak}

The roughness of the underlying density, as measured by $R(f')$ determines the optimal level of smoothing and the accuracy of the histogram estimate.

<br/><br/><br/><br/>

We cannot find the optimal binwidth without known the density $f$ itself.

<br/><br/><br/><br/>

Simple (plug-in) approach: Assume $f$ is a $N(\mu, \sigma^2)$, then

<br/><br/><br/><br/><br/><br/><br/><br/>

[]{.pagebreak}

Data driven approach:


[]{.pagebreak}


# Frequency Polygon

The histogram is simple, useful and piecewise constant.

<br/><br/><br/><br/>

```{r, message=FALSE, warning=FALSE, fig.height=2.5}
library(ISLR)

# optimal h based on normal method
h_0 <- 3.491 * sd(Hitters$Salary, na.rm = TRUE) * sum(!is.na(Hitters$Salary))^(-1/3)

## original histogram with optimal h
ggplot(Hitters) +
  geom_histogram(aes(Salary), binwidth = h_0) -> p

## get values to build freq polygon
vals <- ggplot_build(p)$data[[1]]
poly_dat <- data.frame(x = c(vals$x[1] - h_0, 
                             vals$x, vals$x[nrow(vals)] + h_0),
                       y = c(0, vals$y, 0))

## plot freq polygon
p + geom_line(aes(x, y), data = poly_dat, colour = "red")
```

[]{.pagebreak}

Let $b_1, \dots, b_{K + 1}$ represent bin edges of bins with width $h$ and $n_1, \dots, n_K$ be the number of observations falling into the bins. Let $c_0, \dots, c_{k + 1}$ be the midpoints of the bin interval.

<br/><br/><br/>

The frequency polygon is defined as

<br/><br/><br/><br/><br/><br/>

MISE

<br/><br/><br/><br/><br/><br/>

AMISE

<br/><br/><br/><br/><br/><br/>

Gaussian rule for binwidth


[]{.pagebreak}

```{r, echo = FALSE, warning=FALSE, message=FALSE}
# optimal h based on normal method
h_0 <- 2.15 * sd(Hitters$Salary, na.rm = TRUE) * sum(!is.na(Hitters$Salary))^(-1/5)

## original histogram with optimal h
ggplot(Hitters) +
  geom_histogram(aes(Salary), binwidth = h_0) -> p

## get values to build freq polygon
vals <- ggplot_build(p)$data[[1]]
poly_dat <- data.frame(x = c(vals$x[1] - h_0, 
                             vals$x, vals$x[nrow(vals)] + h_0),
                       y = c(0, vals$y, 0))

## plot freq polygon
ggplot() + geom_line(aes(x, y), data = poly_dat) + xlab("Salary") + ylab("Count")
```


[]{.pagebreak}

In practice, a simple way to construct locally varying binwidth histograms is by transforming the data to a different scale and then smoothing the transformed data. The final estimate is formed by simply transforming the constructed bin edges $\{b_j\}$ back to the original scale.

<br/><br/><br/>

```{r, fig.show='hold', echo=FALSE, warning = FALSE, message=FALSE, fig.height=2}
ggplot() + geom_line(aes(x, y), data = poly_dat) + xlab("Salary") + ylab("Count")

h_0_log <- 2.15 * sd(log(Hitters$Salary, base = 10), na.rm = TRUE) * sum(!is.na(Hitters$Salary))^(-1/5)

## original histogram with optimal h
ggplot(Hitters) +
  geom_histogram(aes(log(Salary, base = 10)), binwidth = h_0_log) -> p_log

## get values to build freq polygon
vals <- ggplot_build(p_log)$data[[1]]
poly_dat_log <- data.frame(x = c(vals$x[1] - h_0_log, 
                                 vals$x, vals$x[nrow(vals)] + h_0_log),
                           y = c(0, vals$y, 0))


## plot freq polygon
ggplot() + geom_line(aes(x, y), data = poly_dat_log) + xlab("log(Salary)") + ylab("Count")

## transform back
poly_dat_bak <- data.frame(x = c(vals$x[1] - h_0_log, 
                                 vals$x, vals$x[nrow(vals)] + h_0_log),
                           y = c(0, vals$y, 0)) |>
  mutate(x = 10^x)

ggplot() + geom_line(aes(x, y), data = poly_dat_bak) + xlab("Salary") + ylab("Count")

```

# Kernel Density Estimation {.page-break-before}

Recall the definition of a density function
$$
f(y) \equiv \frac{d}{dy} F(y) \equiv \lim\limits_{h \rightarrow 0} \frac{F(y + h) - F(y - h)}{2h} = \lim\limits_{h \rightarrow 0} \frac{F(y + h) - F(y)}{h},
$$
where $F(x)$ is the cdf of the random variable $Y$.

<br/><br/><br/><br/>

What if instead, we replace $F(x + h) - F(x - h)$?

\pagebreak

This will weight all points within $h$ of $x$ equally. A univariate *kernel density estimator* will allow a more flexible weighting scheme.

<br/><br/><br/><br/><br/><br/><br/><br/>

Typically, kernel functions are positive everywhere and symmetric about zero.

<br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/>

```{r, echo = FALSE}
x_dat <- rnorm(4, 0, 2.5)

x_grid <- seq(min(x_dat) - diff(range(x_dat)) / 2, max(x_dat) + diff(range(x_dat)) / 2, length.out = 1000)

data.frame(x = x_grid, 
           f_1 = dnorm(x_grid, x_dat[1]) / 4,
           f_2 = dnorm(x_grid, x_dat[2]) / 4,
           f_3 = dnorm(x_grid, x_dat[3]) / 4,
           f_4 = dnorm(x_grid, x_dat[4]) / 4) |>
  mutate(f_hat = f_1 + f_2 + f_3 + f_4) |>
  pivot_longer(-x, names_to = "density", values_to = "f_hat") |>
  ggplot() +
  geom_point(aes(x_dat, 0), data = data.frame(x_dat)) +
  geom_line(aes(x, f_hat, group = density, lty = density)) +
  scale_linetype_manual(values = c(3, 3, 3, 3, 1)) +
  theme(legend.position = "none") +
  ylab("") + xlab("")

```

## Choice of Bandwidth {.page-break-before}

The bandwidth parameter controls the smoothness of the density estimate.

<br/>

The tradeoff that results from choosing the bandwidth + kernel can be quantified through a measure of accuracy of $\hat{f}$, such as MISE.

<br/><br/><br/><br/>

```{r, echo = FALSE}
z <- rbinom(100, 1, 0.5)
x_dat <-  z * rnorm(100, 4) + (1 - z) * rnorm(100, 9, 2)
x_grid <- seq(min(x_dat) - diff(range(x_dat)) / 2, max(x_dat) + diff(range(x_dat)) / 2, length.out = 1000)

h_0 <- 3.491 * sd(x_dat) * (length(x_dat))^(-1/3)

tmp <- density(x_dat, kernel = "gaussian", bw = 1.875, 
               from = min(x_dat) - diff(range(x_dat)) / 2, to = max(x_dat) + diff(range(x_dat)) / 2)
tmp2 <- density(x_dat, kernel = "gaussian", bw = 0.5, 
               from = min(x_dat) - diff(range(x_dat)) / 2, to = max(x_dat) + diff(range(x_dat)) / 2)
tmp3 <- density(x_dat, kernel = "gaussian", bw = 0.1, 
               from = min(x_dat) - diff(range(x_dat)) / 2, to = max(x_dat) + diff(range(x_dat)) / 2)
data.frame(x_1 = tmp$x,
           f_1 = tmp$y,
           x_2 = tmp2$x,
           f_2 = tmp2$y,
           x_3 = tmp3$x,
           f_3 = tmp3$y) |>
  ggplot() +
  geom_histogram(aes(x_dat, y = after_stat(density)), data = data.frame(x_dat), binwidth = h_0) +
  geom_line(aes(x_1, f_1), lty = 1) + 
  geom_line(aes(x_2, f_2), lty = 2) + 
  geom_line(aes(x_3, f_3), lty = 3) + 
  theme(legend.position = "none") +
  ylab("") + xlab("")
```

\newpage

To understand bandwidth selection, let us analyze MISE. Suppose that $K$ is a symmetric, continuous probability density function with mean $0$ and variance $0 < \sigma^2_K < \infty$. Let $R(g) = \int g^2(z) dz$. Recall that

$$
\text{MISE} = \int\text{MSE}(\hat{f}(x)) dx = \qquad\qquad\qquad\qquad\qquad\qquad
$$

Now let $h \rightarrow 0$ and $nh \rightarrow \infty$ as $n \rightarrow \infty$.

\newpage

To minimize AMISE with respect to $h$,

<br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/>

The term $R(f'')$ measures the roughness of the true underlying density. In general, rougher densities are more difficult to estimate and require smaller bandwidth.

The term $[\sigma_K R(K)]^{4/5}$ is a function of the kernel function $K$.

### Cross Validation {.page-break-before}

<br/><br/><br/><br/><br/><br/><br/><br/><br/>

### Plug-in Methods

If the reference density $f$ is Gaussian and a Gaussian kernel $K$ is used, 

<br/><br/><br/><br/>

Empirical estimation of $R(f'')$ may be a better option.

\newpage

## Choice of Kernel

There are two choices we have to make to perform density estimation:

<br/><br/><br/><br/>

### Epanechnikov Kernel

The *Epanechnikov kernel* results from choosing $K$ to minimize $[\sigma_K R(K)]^{4/5}$, restricted to be a symmetric density with finite moments and variance equal to $1$

\newpage

### Canonical Kernels

Unfortunately a particular value of $h$ corresponds to a different amount of smoothing depending on which kernel is being used.

Let $h_K$ and $h_L$ denote the bandwidths that minimize AMISE when using symmetric kernel densities $K$ and $L$. Then,

<br/><br/><br/><br/><br/><br/><br/><br/>

Suppose we rescale a kernel shape so that $h = 1$ corresponds to a bandwidth of $\delta(K)$,

## Bootstrapping and Variability Plot {.page-break-before}

<br/><br/><br/><br/><br/><br/><br/><br/>

```{r, echo = FALSE}
salary_no_na <- Hitters$Salary[!is.na(Hitters$Salary)]
n <- length(salary_no_na)

f_hat <- density(salary_no_na, kernel = "gaussian", bw = "SJ")
f_boot <- data.frame(x = f_hat$x)

eval_density <- function(x, dat, h) {
  n <- length(dat)
  rowSums(sapply(dat, function(dat_i) dnorm((x - dat_i) / h, 0, 1))) / (n * h)
}

for(b in seq_len(1000)) {
  x_star <- salary_no_na[sample(seq_len(n), n, replace = TRUE)]
  f_hat_star <- density(x_star, kernel = "gaussian", bw = "SJ")  
  f_boot[, b + 1] <- eval_density(f_hat$x, x_star, f_hat$bw)
}

data.frame(f_hat$x, f_hat$y, t(apply(f_boot[, -1], 1, quantile, probs = c(.025, .975)))) |>
  ggplot() +
  geom_ribbon(aes(x = f_hat.x, ymin = X2.5., ymax = X97.5.), alpha = 0.5) +
  geom_line(aes(f_hat.x, f_hat.y)) + xlab("Baseball Player Salary") + ylab(expression(hat(f)))


```